{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75980f7d-3327-4340-8d66-73fcc2744e61",
   "metadata": {},
   "source": [
    "# Lab 6 - Convolutional Neural Networks\n",
    "by Rebecca Kuhlman, Sam Yao, and Michael Amberg\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Business Understanding\n",
    "Identifying the type of brain tumor a patient has is an important step in figuring out the treatment plan of a patient. They can be diagnosed via MRI imaging, leading to interest in using machine learning to diagnose the patient. Having a second opinion on brain tumor diagnoses would help improve patient care and outcomes, and lessen stress on doctors. A machine learning model could also speed up analysis time and pick out which patients are in need of urgent treatment.\n",
    "In this dataset, there is glioma, meningioma, and pituitary tumors, as well as MRI images with no tumors. Glioma tumors are usually malignant, while meningioma and pituitary tumors are usually benign. Different types of tumors are made of different types of cells and have a location where they are most likely to be located. More information can be found at: https://www.mayoclinic.org/diseases-conditions/brain-tumor/symptoms-causes/syc-20350084\n",
    "There are many other types of tumors that future algorithms will be need to address. The majority of other types of tumors are more common in children, while the set we are dealing with are all adult brain images.\n",
    "Because the model deals with health conditions that have extreme affects on the patient, model accuracy is extremely important. Furthermore, accuracy must fine-tuned to avoid fatal misdiagnosis. While incorrectly marking a patient with a benign tumor as malignant is wasteful, the adverse affects are minimal. Inversely, misdiagnosing a malignant tumor as benign may have fatal effects for the patient. Therefore, the designed model must minimize the rate of false negatives with accuracy of 95% or more.\n",
    "It should be noted that the majority of misdiagnose of brain tumors happen before a brain scan or related test is ordered. https://paulandperkins.com/brain-tumors/"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "fd39dd5f-dd06-43f1-bef8-d848a1ade022",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2a1eb6-f1e6-4a3f-8695-ea35db15d52f",
   "metadata": {},
   "source": [
    "[1.5 points] \n",
    "Choose and explain what metric(s) you will use to evaluate your algorithmâ€™s performance. You should give a detailed argument for why this (these) metric(s) are appropriate on your data. That is, why is the metric appropriate for the task (e.g., in terms of the business case for the task). Please note: rarely is accuracy the best evaluation metric to use. Think deeply about an appropriate measure of performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7357d9e8-c526-44d4-a1cc-7237de733f0c",
   "metadata": {},
   "source": [
    "Because we will be dealing with identifying brain tumors, we want to use Recall. The equation for Recall takes into account False Negatives, which would be very bad if you falsely cleared someone of brain tumors, but they did in fact have a tumor. This will be a high stakes identification, so at the very least our recall score should be 85% accuarate to be deployed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324c547d-94a5-43fc-ba4c-bc85beff2860",
   "metadata": {},
   "source": [
    "[1.5 points] Choose the method you will use for dividing your data into training and testing (i.e., are you using Stratified 10-fold cross validation? Shuffle splits? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. Convince me that your cross validation method is a realistic mirroring of how an algorithm would be used in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53079ff1-9b88-4009-9e8a-47e6a59aa65a",
   "metadata": {},
   "source": [
    "We could try Stratified 10 k-fold validation, because it seemed to be effective from the results we had in the last lab. I have no idea if this would extend to image data, but we could give it a try.\n",
    "Stratified 10 k-fold validation is most effective with small amounts of imbalanced data. We have to think about balance a lot in our data as our tumor categories will have differing likelihoods, and we have a lot of different types of MRI photos.\n",
    "In a deployment setting, different tumors (or when we are actually getting a tumor) will come up at different rates. There are many types of tumors with different subcategories, we will only be training our program for 3 types of brain tumors. Our program must be robust under these uneven circumstances. Stratified 10 k-fold validation would be one way to address this.\n",
    "\n",
    "https://www.analyseup.com/python-machine-learning/stratified-kfold.html\n",
    "https://www.aans.org/en/Patients/Neurosurgical-Conditions-and-Treatments/Brain-Tumors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a202e22-7402-4b36-bfff-0d9ff1be7e56",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b76140d-5556-429a-84e7-2f7b507f4be0",
   "metadata": {},
   "source": [
    "[1.5 points]  Setup the training to use data expansion in Keras (also called data augmentation). Explain why the chosen data expansion techniques are appropriate for your dataset. You can use the keras ImageGenerator as a pre-processing step OR in the optimization loop. You can also use the Keras-cv augmenter (a separate package: https://keras.io/keras_cv/ Links to an external site.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa1c8391-544a-4d35-8b87-673f9e17aa44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1876 files belonging to 4 classes.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'_BatchDataset' object has no attribute 'astype'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 8\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m      6\u001B[0m df_training \u001B[38;5;241m=\u001B[39m keras\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mimage_dataset_from_directory(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./Training\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m      7\u001B[0m                                                       image_size\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m512\u001B[39m, \u001B[38;5;241m512\u001B[39m))\n\u001B[1;32m----> 8\u001B[0m df_training \u001B[38;5;241m=\u001B[39m \u001B[43mdf_training\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mastype\u001B[49m(np\u001B[38;5;241m.\u001B[39mint32)\n",
      "\u001B[1;31mAttributeError\u001B[0m: '_BatchDataset' object has no attribute 'astype'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "df_training = keras.utils.image_dataset_from_directory(\"./Training\",\n",
    "                                                      image_size=(512, 512))\n",
    "df_training = df_training.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef10d7c9-be7c-47b5-9076-0d02cf688505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.66 GiB for an array with shape (1876, 262144) and data type object",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 30\u001B[0m\n\u001B[0;32m     26\u001B[0m     img_data_array \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masarray(img_data_array, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mndarray)\n\u001B[0;32m     27\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img_data_array, class_name\n\u001B[1;32m---> 30\u001B[0m df_training, training_classes \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m./Training\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     31\u001B[0m df_testing, testing_classes \u001B[38;5;241m=\u001B[39m create_dataset(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./Testing\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[2], line 26\u001B[0m, in \u001B[0;36mcreate_dataset\u001B[1;34m(img_folder)\u001B[0m\n\u001B[0;32m     24\u001B[0m         class_name\u001B[38;5;241m.\u001B[39mappend(dir1)\n\u001B[0;32m     25\u001B[0m \u001B[38;5;66;03m# return array with training data.\u001B[39;00m\n\u001B[1;32m---> 26\u001B[0m img_data_array \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43masarray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg_data_array\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mndarray\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m img_data_array, class_name\n",
      "\u001B[1;31mMemoryError\u001B[0m: Unable to allocate 3.66 GiB for an array with shape (1876, 262144) and data type object"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image  # Utilized Source [2]\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# This method creates the data, whether training or testing, in the form we desire\n",
    "# Uses code from source [2] to create the training datasets\n",
    "def create_dataset(img_folder):\n",
    "    # Read through all files in \"./Training\"\n",
    "    img_data_array = []\n",
    "    class_name = []\n",
    "\n",
    "    for dir1 in os.listdir(img_folder):\n",
    "        for file in os.listdir(os.path.join(img_folder, dir1)):\n",
    "            image_path = os.path.join(img_folder, dir1, file)\n",
    "            image = np.array(Image.open(image_path).convert(\"L\").resize((512, 512)))\n",
    "\n",
    "            image = image.reshape((1, 262144))  #Vectorizes each image\n",
    "            image = image.astype('float64')\n",
    "            image /= 255  #Normalize Values\n",
    "            img_data_array.append(image[0])\n",
    "            class_name.append(dir1)\n",
    "    # return array with training data.\n",
    "    img_data_array = np.asarray(img_data_array, dtype=np.ndarray)\n",
    "    return img_data_array, class_name\n",
    "\n",
    "\n",
    "df_training, training_classes = create_dataset(\"./Training\")\n",
    "df_testing, testing_classes = create_dataset(\"./Testing\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "NUM_CLASSES = 4\n",
    "\n",
    "# make a 3 layer keras MLP\n",
    "mlp = Sequential()\n",
    "mlp.add( Flatten() ) # make images flat for the MLP input\n",
    "mlp.add( Dense(input_dim=1, units=30,\n",
    "               activation='relu') )\n",
    "mlp.add( Dense(units=15, activation='relu') )\n",
    "mlp.add( Dense(NUM_CLASSES) )\n",
    "mlp.add( Activation('softmax') )\n",
    "\n",
    "mlp.compile(loss='mean_squared_error',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['recall'])\n",
    "\n",
    "mlp.fit(df_training,\n",
    "        batch_size=32, epochs=150,\n",
    "        shuffle=True, verbose=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# make a CNN with conv layer and max pooling\n",
    "cnn = Sequential()\n",
    "cnn.add( Conv2D(filters=16, kernel_size= (2, 2), padding='same',\n",
    "                input_shape=(8,8,1),\n",
    "               ) )\n",
    "\n",
    "cnn.add( MaxPooling2D(pool_size=(2, 2)) )\n",
    "cnn.add( Activation('relu') )\n",
    "# add one layer on flattened output\n",
    "cnn.add( Flatten() )\n",
    "cnn.add( Dense(NUM_CLASSES) )\n",
    "cnn.add( Activation('softmax') )\n",
    "\n",
    "cnn.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " Explain why the chosen data expansion techniques are appropriate for your dataset. :\n",
    " We used __ data expansion cause __"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "    Create a convolutional neural network to use on your data using Keras. Investigate at least two different convolutional network architectures (and investigate changing some parameters of each architecture such as the number of filters--at minimum have two variations of each network for a total of four models trained). Use the method of train/test splitting and evaluation metric that you argued for at the beginning of the lab. Visualize the performance of the training and validation sets per iteration (use the \"history\" parameter of Keras). Be sure that models converge.\n",
    "    [1.5 points] Visualize the final results of the CNNs and interpret/compare the performances. Use proper statistics as appropriate, especially for comparing models.\n",
    "    [1 points] Compare the performance of your convolutional network to a standard multi-layer perceptron (MLP) using the receiver operating characteristic and area under the curve. Use proper statistical comparison techniques.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import metrics as mt\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "def compare_mlp_cnn(cnn, mlp, X_test, y_test, labels='auto'):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    if cnn is not None:\n",
    "        yhat_cnn = np.argmax(cnn.predict(X_test), axis=1)\n",
    "        acc_cnn = mt.accuracy_score(y_test,yhat_cnn)\n",
    "        plt.subplot(1,2,1)\n",
    "        cm = mt.confusion_matrix(y_test,yhat_cnn)\n",
    "        cm = cm/np.sum(cm,axis=1)[:,np.newaxis]\n",
    "        sns.heatmap(cm, annot=True, fmt='.2f',xticklabels=labels,yticklabels=labels)\n",
    "        plt.title('CNN: '+str(acc_cnn))\n",
    "\n",
    "    if mlp is not None:\n",
    "        yhat_mlp = np.argmax(mlp.predict(X_test), axis=1)\n",
    "        acc_mlp = mt.accuracy_score(y_test,yhat_mlp)\n",
    "        plt.subplot(1,2,2)\n",
    "        cm = mt.confusion_matrix(y_test,yhat_mlp)\n",
    "        cm = cm/np.sum(cm,axis=1)[:,np.newaxis]\n",
    "        sns.heatmap(cm,annot=True, fmt='.2f',xticklabels=labels,yticklabels=labels)\n",
    "        plt.title('MLP: '+str(acc_mlp))\n",
    "\n",
    "compare_mlp_cnn(cnn,mlp,X_test,y_test)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Exceptional Work (1 points total)\n",
    "Use transfer learning to pre-train the weights of your initial layers of your CNN. Compare the performance when using transfer learning to training without transfer learning (i.e., compare to your best model from above) in terms of classification performance."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
